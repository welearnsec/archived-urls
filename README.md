# ğŸ•µï¸â€â™‚ï¸ archive_urls

**archive_urls** is a Python OSINT tool that collects archived and public URLs for a given domain using multiple public sources:

- ğŸ“¦ Wayback Machine  
- ğŸ” urlscan.io  
- ğŸ›°ï¸ Common Crawl  
- ğŸŒ AlienVault OTX  

Useful for bug bounty hunters, penetration testers, and recon specialists. It supports deduplication, output formatting (HTML/CSV/TXT), and grouping similar URLs.

---

## ğŸ“Œ Features

- ğŸ”— Collect URLs from 4 major OSINT sources
- ğŸ§  Optional deduplication with `--unique`
- ğŸ¤ Group similar URLs using `--group-similar`
- ğŸ’¾ Save results in HTML, CSV, and TXT
- ğŸ“… Timestamped filenames per domain
- ğŸ” Highlights URLs with query parameters
- ğŸ’» Simple CLI-based tool (no API keys needed)

---

## ğŸ’» Installation

```bash
git clone https://github.com/yourusername/archive_urls.git
cd archive_urls
pip install requests, beautifulsoup, tldextract

** Python 3.7 or later is required.**


**python archive_urls.py <target-domain> --format=html,csv,txt [--unique] [--group-similar]**
python archive_urls.py example.com --format=html
python archive_urls.py example.com --format=csv,txt --unique
python archive_urls.py testsite.org --format=html,csv --group-similar

| Argument          | Description                                                    |
| ----------------- | -------------------------------------------------------------- |
| `<target-domain>` | Domain to search archived URLs for                             |
| `--format=`       | Comma-separated formats: `html`, `csv`, `txt`                  |
| `--unique`        | (Optional) Removes exact duplicate URLs                        |
| `--group-similar` | (Optional) Groups URLs with same path but different parameters |



ğŸ™ Credits
Wayback Machine
urlscan.io
Common Crawl
AlienVault OTX




